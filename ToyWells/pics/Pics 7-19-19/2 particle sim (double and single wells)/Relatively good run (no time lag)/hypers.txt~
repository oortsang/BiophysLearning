time_lagged = False
variational = True

if time_lagged:
    sim_data = tla_sim_data
else:
    sim_data = nor_sim_data

n_epochs = 30
batch_size = 100

# Network dimensions
# in_dim = 8 # input dimension
in_dim = sim_data.data[:].shape[1]
if time_lagged:
    in_dim //= 2

h_size = in_dim+1 # size of hidden layers -- don't have to all be the same size though!
n_z    = 1 # dimensionality of latent space

# the layers themselves
encode_layers_means = [nn.Linear(in_dim, n_z),
                       # nn.ReLU(),
                       # nn.Linear(h_size, n_z)
                       # just linear combination without activation
                      ]
encode_layers_vars  = [nn.Linear(in_dim, h_size),
                       nn.ReLU(),
                       nn.Linear(h_size, n_z),
                      ]
decode_layers       = [nn.Linear(n_z, in_dim),
                       # nn.ReLU(),
                       # nn.Linear(h_size,in_dim)
                      ]

## Learning Algorithm Hyperparameters
optim_fn = optim.Adam
lr = 5e-3
weight_decay = 4e-4
kl_lambda = 0.1
